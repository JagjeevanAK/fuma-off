---
title: Get robots.txt file
full: true
_openapi:
  method: GET
  route: /robots.txt
  toc: []
  structuredData:
    headings: []
    contents:
      - content: >
          Returns the robots.txt file that disallows all web crawlers to prevent
          excessive requests.

          This is a standard file used by web crawlers to understand crawling
          permissions.
---

{/* This file was generated by Fumadocs. Do not edit this file directly. Any changes should be made by running the generation command again. */}

Returns the robots.txt file that disallows all web crawlers to prevent excessive requests.
This is a standard file used by web crawlers to understand crawling permissions.


<APIPage document={"specfiles-json/robotoff-openapi.json"} operations={[{"path":"/robots.txt","method":"get"}]} webhooks={[]} hasHead={false} />